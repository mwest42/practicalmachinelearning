---
title: "Practical Machine Learning Course Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

The Weight Lifting Exercises dataset from http://groupware.les.inf.puc-rio.br/har was analyzed for this project.  The objective was to use machine learning to generate a model to determine how participants were performing biceps curl: with proper technique, or using one of four different incorrect methods.


## Cleaning Data

The WLE paper (Velloso, et al) mentions that some of the data is a summary of the other columns.  Loading in the training data set, the number of NAs in each column are counted.

```{r cleaning, eval=TRUE}
rawdata <- read.csv("pml-training.csv")
countnas <- data.frame(totalna = sapply(rawdata,function(y) sum(is.na(y))))


plot(countnas$totalna, type="p", ylab= "NAs per row", main = "NA count")
```

This reveals that each column either has no NAs, or nearly the entire column is NA.  These columns are lef out of the model.  Similarly, there are also many columns that have a high number of blanks, which are also excluded.  The first five columns are also excluded: the first appears to be an index, and the same timestamps probably wouldn't be repeated in the test set.


```{r column choice}
nonna <- row.names(subset(countnas,totalna == 0))

blanks <- data.frame(totblanks = sapply(rawdata,function(y) sum(y=="")))
nonblank <- row.names(subset(blanks,blanks == 0))

wledata<- rawdata[,nonna]
wledata <- wledata[,nonblank]
wledata <- wledata[,-c(1:5)]
```


## Cross-validation

10-fold cross validation was used in the model selection.  This was done by using the trainControl parameter within the train option, as an alternative to the default method of bootstrapping.

```{r model fit}
library(caret)
mod <- train(classe~.,data = wledata,method = "rf", trControl = trainControl
             (method = "cv", number = 10), ntree=20)
```


## Model Selection

The random forest method was chosen to create a classification model to determine the "classe" of the type of movement.  The number of trees was limited to 20 in order to limit the time it would take to generate the model.

Plotting the finalModel shows that the error might still decrease past ntree=20, but the error is low nonetheless. 

```{r trees, eval =TRUE}
plot(mod$finalModel, main = "Error by # of Trees Used")
```

The train function also tunes for the number of variable that should be considered at each node of the tree.  In this case, the number of variabes is:

```{r mtry, eval = TRUE}
mod$finalModel$mtry
```

In this graph you can see the difference in error rate for a different number of predictor variables used.

```{r mtry plot, eval = TRUE}
plot(mod, main = "Accuracy per # of predictors")
```

## Out of Sample Error

The out of sample error is estimated by the average accuracy of the cross-validation sets:

```{r cv Accuract, eval = TRUE}
mean(mod$resample$Accuracy)
```

There's also the accuracy of the model on the entire training set:

```{r Accuracy, eval=TRUE}

confusionMatrix(predict(mod,wledata),wledata$classe)$overall['Accuracy']
```


## Prediction

To predict the classe of the values in the test set, the columns are subset as they were for the training data.  Then the predict function is run with the model and the test data.

```{r predict, eval=FALSE}
testraw <- read.csv("pml-testing.csv")
tnonna <- nonna[nonna !="classe"]
tnonblank <- nonblank[nonblank != "classe"]

testing <- testraw[,tnonna]
testing <- testing[,tnonblank]
testing <- testing[,-1]

predict(mod,testing)
```


## Citations

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013
